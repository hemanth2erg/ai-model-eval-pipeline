# ai-model-eval-pipeline
Automated LLM scoring with rubrics and response logging
An end-to-end automated pipeline for evaluating LLM outputs using rubric-based scoring, response clustering, and human-in-the-loop correction.

## Features
- Modular evaluation for hallucinations, logical errors, and factual inaccuracies
- Pluggable rubric system
- MongoDB logging and FastAPI interface
- Designed for scale with Docker and queue-based async processing

## Tech Stack
Python, FastAPI, MongoDB, Docker, HuggingFace Transformers



## Author
Hemanth Matt
